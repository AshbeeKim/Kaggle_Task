# -*- coding: utf-8 -*-
"""[otto]noDataPreprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1REEG9pXK1-3jLD33Zv1hz_nUDHqJnXQ7
"""

!pip install seaborn
!pip install matplotlib
!pip install xgboost
!pip install sklearn
!pip install mglearn

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib
from matplotlib import pyplot as plt
import sklearn
import mglearn
import graphviz
import warnings
warnings.filterwarnings('ignore')
#VS에서 사용을 하기 위해서는 CUDA를 설치해야 함(CUDA_11보다는 10이 안정화되어있음)
#딥러닝 넘어갈 때, CUDA를 깔아야 함__경로 겁나 많이 꼬임

"""# EDA"""

# #구글 드라이브에 캐글 데이터 다운로드
import os  
os.environ['KAGGLE_CONFIG_DIR'] = "/content/drive/MyDrive/data/kaggle"   
from google.colab import drive  
drive.mount('/content/drive')
!cd /content/drive/MyDrive/data/kaggle  
!kaggle competitions download -c otto-group-product-classification-challenge
!unzip train.csv.zip
!unzip test.csv.zip
!unzip sampleSubmission.csv.zip
!ls

!pwd
!ls
train = pd.read_csv('./train.csv')
test = pd.read_csv('./test.csv')
sampleSubmission = pd.read_csv('./sampleSubmission.csv')

trn = train.copy()
tst = test.copy()
sS = sampleSubmission.copy()

#결측치
print(train.isna().sum())
print(test.isna().sum())
print(train.isnull().sum())
print(test.isnull().sum())
# print(sampleSubmission.isna().sum())
# they don't have NULL

#drop id
excid_t = trn.drop('id',axis=1)
#drop target
exctg_t = trn.drop('target',axis=1)
#drop id and target
excboth_t = trn.drop(['id','target'],axis=1)

onlyvalues = trn.values

print(excid_t.shape)
print(exctg_t.shape)
print(excboth_t.shape)
print(onlyvalues.shape)

trn['target'].unique()

def class_count_sum(trn):
  for i in range(1,10):
    print(trn[trn['target']==f'Class_{i}'].count().sum())
class_count_sum(trn)

# print(trn[trn['target']=='Class_1'].count().sum()) #183255
# print(trn[trn['target']=='Class_2'].count().sum()) #1531590
# print(trn[trn['target']=='Class_3'].count().sum()) #760380
# print(trn[trn['target']=='Class_4'].count().sum()) #255645
# print(trn[trn['target']=='Class_5'].count().sum()) #260205
# print(trn[trn['target']=='Class_6'].count().sum()) #1342825
# print(trn[trn['target']=='Class_7'].count().sum()) #269705
# print(trn[trn['target']=='Class_8'].count().sum()) #804080
# print(trn[trn['target']=='Class_9'].count().sum()) #470725

plt.title('target counts sum')
plt.xlabel('class')
sns.countplot(trn['target'], saturation=0.6, data=excid_t, dodge=True, palette = 'inferno')

"""# Label Encoding"""

from sklearn.preprocessing import LabelEncoder
LE=LabelEncoder()
trn['target'] = LE.fit_transform(trn['target'])+1
tg = trn['target']
print(tg)

features = excboth_t
target = tg
feat = trn.columns[1:-1]

print(features)
print('='*80)
print(target)
print('='*80)
print(feat)

print(trn['feat_1'].max())

def feat_max(trn):
  for i in range(1,94):
    fcol = 'f'+str(i)+'max'
    fv = np.array(trn['feat_'+str(i)].max())
    print(fcol,':',fv)
feat_max(trn)

corrnoid = excid_t.corr().round(2)
print(corrnoid)
plt.figure(figsize=(20,20))
plt.imshow(corrnoid, cmap='magma')

corrcls1 = trn[trn['target']==1].corr().round(2)
corrcls2 = trn[trn['target']==2].corr().round(2)
corrcls3 = trn[trn['target']==3].corr().round(2)
corrcls4 = trn[trn['target']==4].corr().round(2)
corrcls5 = trn[trn['target']==5].corr().round(2)
corrcls6 = trn[trn['target']==6].corr().round(2)
corrcls7 = trn[trn['target']==7].corr().round(2)
corrcls8 = trn[trn['target']==8].corr().round(2)
corrcls9 = trn[trn['target']==9].corr().round(2)

fig, axs = plt.subplots(3,3,figsize=(10,10))
axs[0,0].imshow(corrcls1, cmap='inferno',interpolation='nearest')
axs[1,0].imshow(corrcls2, cmap='inferno',interpolation='nearest')
axs[2,0].imshow(corrcls3, cmap='inferno',interpolation='nearest')
axs[0,1].imshow(corrcls4, cmap='inferno',interpolation='nearest')
axs[1,1].imshow(corrcls5, cmap='inferno',interpolation='nearest')
axs[2,1].imshow(corrcls6, cmap='inferno',interpolation='nearest')
axs[0,2].imshow(corrcls7, cmap='inferno',interpolation='nearest')
axs[1,2].imshow(corrcls8, cmap='inferno',interpolation='nearest')
axs[2,2].imshow(corrcls9, cmap='inferno',interpolation='nearest')

axs[0,0].set_title('corrcls1')
axs[1,0].set_title('corrcls2')
axs[2,0].set_title('corrcls3')
axs[0,1].set_title('corrcls4')
axs[1,1].set_title('corrcls5')
axs[2,1].set_title('corrcls6')
axs[0,2].set_title('corrcls7')
axs[1,2].set_title('corrcls8')
axs[2,2].set_title('corrcls9')
plt.show()

gtarget = excid_t.groupby('target')
gtarget.max()

"""# Metrics__split data, load predict data"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

Y = tg
X = trn.drop(['id','target'],axis=1)
# X = trn.drop('target',axis=1)
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 42)
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import roc_curve, auc

"""# Visualization"""

# Commented out IPython magic to ensure Python compatibility.
from IPython.display import display
# %matplotlib inline

import matplotlib.pylab as pylab
params = {'legend.fontsize': 'x-large',
         'axes.labelsize': 'x-large',
         'axes.titlesize':'x-large',
         'xtick.labelsize':'x-large',
         'ytick.labelsize':'x-large'}
pylab.rcParams.update(params)

from mglearn.plots import plot_2d_classification

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier
KNN = KNeighborsClassifier(n_neighbors = 9)
X_train_KNN = X_train.copy()
Y_train_KNN = Y_train.copy()
X_test_KNN = X_test.copy()
Y_test_KNN = Y_test.copy()

KNN.fit(X_train_KNN,Y_train_KNN)
print(KNN.score(X_train_KNN,Y_train_KNN))
print(KNN.score(X_test_KNN,Y_test_KNN))

print('accuracy : {:.2f}'.format(KNN.score(X_test_KNN,Y_test_KNN)))

"""# KFold
*  colab에서는 안되나봄...ㅜ
"""

from sklearn.model_selection import KFold

# KFD = KFold(n_splits=10,random_state=42)
# LR = LogisticRegression()
# X_KFD = X.copy()
# Y_KFD = Y.copy()
# results_KFD = cross_val_score(LR,X,Y,cv=KFD)
# print(f'Accuracy : {results_KFD.mean()*100:.3f}({results_KFD.std()*100:.3f})')

"""# Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

GNB = GaussianNB()

X_train_GNB = X_train.copy()
Y_train_GNB = Y_train.copy()
X_test_GNB = X_test.copy()
Y_test_GNB = Y_test.copy()

GNB.fit(X_train_GNB, Y_train_GNB)
print(GNB.score(X_train_GNB,Y_train_GNB))
print(GNB.score(X_test_GNB,Y_test_GNB))

y_pred_GNB = GNB.predict(X_test_GNB)
print("Accuracy:", metrics.accuracy_score(Y_test_GNB, y_pred_GNB))

"""# Decision Tree"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import DecisionTreeClassifier

DTL = DecisionTreeRegressor()
DTC = DecisionTreeClassifier()

X_train_DTC = X_train.copy()
Y_train_DTC = Y_train.copy()
X_test_DTC = X_test.copy()
Y_test_DTC = Y_test.copy()

DTL.fit(X_train_DTC,Y_train_DTC)
print(DTL.score(X_train_DTC,Y_train_DTC))
print(DTL.score(X_test_DTC,Y_test_DTC))

DTC.fit(X_train_DTC,Y_train_DTC)
print(DTC.score(X_train_DTC,Y_train_DTC))
print(DTC.score(X_test_DTC,Y_test_DTC))

y_pred_DTL = DTL.predict(X_test_DTC)
y_pred_DTC = DTC.predict(X_test_DTC)
print(f'GBC Accuracy : {metrics.accuracy_score(Y_test_DTC,y_pred_DTL)}')
print(f'GBN Accuracy : {metrics.accuracy_score(Y_test_DTC,y_pred_DTC)}')

# mglearn.plots.plot_tree_progressive()

# from sklearn.tree import export_graphviz
# export_graphviz(tree, out_file="DTC.dot", class_names=tg,
#                 feature_names=feat, impurity=False, filled=True)

# with open("DTC.dot") as f:
#     dot_graph = f.read()
# display(graphviz.Source(dot_graph))

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

RFC = RandomForestClassifier(bootstrap=False)
X_train_RFC = X_train.copy()
Y_train_RFC = Y_train.copy()
X_test_RFC = X_test.copy()
Y_test_RFC = Y_test.copy()
RFC.fit(X_train_RFC,Y_train_RFC)
print(RFC.score(X_train_RFC,Y_train_RFC))
print(RFC.score(X_test_RFC,Y_test_RFC))

RF1 = RandomForestClassifier(n_estimators=9, n_jobs=1, random_state=42)
RF1.fit(X_train_RFC,Y_train_RFC)
print(RF1.score(X_train_RFC,Y_train_RFC))
print(RF1.score(X_test_RFC,Y_test_RFC))

RF2 = RandomForestClassifier(n_estimators=18, n_jobs=1, random_state=42)
RF2.fit(X_train_RFC,Y_train_RFC)
print(RF2.score(X_train_RFC,Y_train_RFC))
print(RF2.score(X_test_RFC,Y_test_RFC))

RF3 = RandomForestClassifier(n_estimators=27, n_jobs=1, random_state=42)
RF3.fit(X_train_RFC,Y_train_RFC)
print(RF3.score(X_train_RFC,Y_train_RFC))
print(RF3.score(X_test_RFC,Y_test_RFC))

RF4 = RandomForestClassifier(n_estimators=45, n_jobs=1, random_state=42)
RF4.fit(X_train_RFC,Y_train_RFC)
print(RF4.score(X_train_RFC,Y_train_RFC))
print(RF4.score(X_test_RFC,Y_test_RFC))

RF5 = RandomForestClassifier(n_estimators=62, n_jobs=1, random_state=42)
RF5.fit(X_train_RFC,Y_train_RFC)
print(RF5.score(X_train_RFC,Y_train_RFC))
print(RF5.score(X_test_RFC,Y_test_RFC))

RFX = RandomForestClassifier(n_estimators=81,n_jobs=1, random_state=42)
RFX.fit(X_train_RFC,Y_train_RFC)
#should consider overfitting
#so far, 81 to 90 get same RF
# e : 62, j : 1, r : 42-->

print(RFX.score(X_train_RFC,Y_train_RFC))
print(RFX.score(X_test_RFC,Y_test_RFC))

y_pred_RFC = RFC.predict(X_test_RFC)
y_pred_RF1 = RF1.predict(X_test_RFC)
y_pred_RF2 = RF2.predict(X_test_RFC)
y_pred_RF3 = RF3.predict(X_test_RFC)
y_pred_RF4 = RF4.predict(X_test_RFC)
y_pred_RF5 = RF5.predict(X_test_RFC)
y_pred_RFX = RFX.predict(X_test_RFC)

print(f'RFC Accuracy : {metrics.accuracy_score(Y_test_RFC,y_pred_RFC).round(2)}')
print(f'RF1 Accuracy : {metrics.accuracy_score(Y_test_RFC,y_pred_RF1).round(2)}')
print(f'RF2 Accuracy : {metrics.accuracy_score(Y_test_RFC,y_pred_RF2).round(2)}')
print(f'RF3 Accuracy : {metrics.accuracy_score(Y_test_RFC,y_pred_RF3).round(2)}')
print(f'RF4 Accuracy : {metrics.accuracy_score(Y_test_RFC,y_pred_RF4).round(2)}')
print(f'RF5 Accuracy : {metrics.accuracy_score(Y_test_RFC,y_pred_RF5).round(2)}')
print(f'RFX Accuracy : {metrics.accuracy_score(Y_test_RFC,y_pred_RFX).round(2)}')

# from sklearn.ensemble import export_graphviz
# export_graphviz(tree, out_file="RF.dot", class_names=tg,
#                 feature_names=feat, impurity=False, filled=True)

# with open("RF.dot") as f:
#     dot_graph = f.read()
# display(graphviz.Source(dot_graph))

"""# Ada Boosting"""

from sklearn.ensemble import AdaBoostClassifier

ABC = AdaBoostClassifier()

X_train_ABC = X_train.copy()
Y_train_ABC = Y_train.copy()
X_test_ABC = X_test.copy()
Y_test_ABC = Y_test.copy()

ABC.fit(X_train_ABC,Y_train_ABC)
print(ABC.score(X_train_ABC,Y_train_ABC))
print(ABC.score(X_test_ABC,Y_test_ABC))

y_pred_ABC = ABC.predict(X_test_ABC)
print(f'Accuracy : {metrics.accuracy_score(Y_test_ABC,y_pred_ABC)}')

# from sklearn.ensemble import export_graphviz
# export_graphviz(tree, out_file="XGB.dot", class_names=tg,
#                 feature_names=feat, impurity=False, filled=True)

# with open("XGB.dot") as f:
#     dot_graph = f.read()
# display(graphviz.Source(dot_graph))

"""# GradientBoost"""

from sklearn.ensemble import GradientBoostingClassifier

X_train_GBC = X_train.copy()
Y_train_GBC = Y_train.copy()
X_test_GBC = X_test.copy()
Y_test_GBC = Y_test.copy()

GBC = GradientBoostingClassifier(random_state=0) 
GBC.fit(X_train_GBC, Y_train_GBC)
print(GBC.score(X_train_GBC,Y_train_GBC))
print(GBC.score(X_test_GBC,Y_test_GBC))

GBN = GradientBoostingClassifier(random_state=0,max_depth=3, learning_rate=0.01) 
GBN.fit(X_train_GBC, Y_train_GBC)
print(GBN.score(X_train_GBC,Y_train_GBC))
print(GBN.score(X_test_GBC,Y_test_GBC))

y_pred_GBC = GBC.predict(X_test_GBC)
y_pred_GBN = GBN.predict(X_test_GBC)
print(f'GBC Accuracy : {metrics.accuracy_score(Y_test_GBC,y_pred_GBC)}')
print(f'GBN Accuracy : {metrics.accuracy_score(Y_test_GBC,y_pred_GBN)}')

"""# XGBoost"""

import xgboost as xgb

XGB = xgb()

# from sklearn.ensemble import export_graphviz
# export_graphviz(tree, out_file="XGB.dot", class_names=tg,
#                 feature_names=feat, impurity=False, filled=True)

# with open("XGB.dot") as f:
#     dot_graph = f.read()
# display(graphviz.Source(dot_graph))